{
  "identity": {
    "file_name": "identity.json"
  },
  "alpaca_en_demo": {
    "file_name": "alpaca_en_demo.json"
  },
  "alpaca_zh_demo": {
    "file_name": "alpaca_zh_demo.json"
  },
  "glaive_toolcall_en_demo": {
    "file_name": "glaive_toolcall_en_demo.json",
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations",
      "tools": "tools"
    }
  },
  "glaive_toolcall_zh_demo": {
    "file_name": "glaive_toolcall_zh_demo.json",
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations",
      "tools": "tools"
    }
  },
  "mllm_demo": {
    "file_name": "mllm_demo.json",
    "formatting": "sharegpt",
    "columns": {
      "messages": "messages",
      "images": "images"
    },
    "tags": {
      "role_tag": "role",
      "content_tag": "content",
      "user_tag": "user",
      "assistant_tag": "assistant"
    }
  },
  "mllm_audio_demo": {
    "file_name": "mllm_audio_demo.json",
    "formatting": "sharegpt",
    "columns": {
      "messages": "messages",
      "audios": "audios"
    },
    "tags": {
      "role_tag": "role",
      "content_tag": "content",
      "user_tag": "user",
      "assistant_tag": "assistant"
    }
  },
  "mllm_video_demo": {
    "file_name": "mllm_video_demo.json",
    "formatting": "sharegpt",
    "columns": {
      "messages": "messages",
      "videos": "videos"
    },
    "tags": {
      "role_tag": "role",
      "content_tag": "content",
      "user_tag": "user",
      "assistant_tag": "assistant"
    }
  },
  "mllm_video_audio_demo": {
    "file_name": "mllm_video_audio_demo.json",
    "formatting": "sharegpt",
    "columns": {
      "messages": "messages",
      "videos": "videos",
      "audios": "audios"
    },
    "tags": {
      "role_tag": "role",
      "content_tag": "content",
      "user_tag": "user",
      "assistant_tag": "assistant"
    }
  },
  "alpaca_en": {
    "hf_hub_url": "llamafactory/alpaca_en",
    "ms_hub_url": "llamafactory/alpaca_en",
    "om_hub_url": "HaM/alpaca_en"
  },
  "alpaca_zh": {
    "hf_hub_url": "llamafactory/alpaca_zh",
    "ms_hub_url": "llamafactory/alpaca_zh"
  },
  "alpaca_gpt4_en": {
    "hf_hub_url": "llamafactory/alpaca_gpt4_en",
    "ms_hub_url": "llamafactory/alpaca_gpt4_en"
  },
  "alpaca_gpt4_zh": {
    "hf_hub_url": "llamafactory/alpaca_gpt4_zh",
    "ms_hub_url": "llamafactory/alpaca_gpt4_zh",
    "om_hub_url": "State_Cloud/alpaca-gpt4-data-zh"
  },
  "glaive_toolcall_en": {
    "hf_hub_url": "llamafactory/glaive_toolcall_en",
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations",
      "tools": "tools"
    }
  },
  "glaive_toolcall_zh": {
    "hf_hub_url": "llamafactory/glaive_toolcall_zh",
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations",
      "tools": "tools"
    }
  },
  "lima": {
    "hf_hub_url": "llamafactory/lima",
    "formatting": "sharegpt"
  },
  "guanaco": {
    "hf_hub_url": "JosephusCheung/GuanacoDataset",
    "ms_hub_url": "AI-ModelScope/GuanacoDataset"
  },
  "belle_2m": {
    "hf_hub_url": "BelleGroup/train_2M_CN",
    "ms_hub_url": "AI-ModelScope/train_2M_CN"
  },
  "belle_1m": {
    "hf_hub_url": "BelleGroup/train_1M_CN",
    "ms_hub_url": "AI-ModelScope/train_1M_CN"
  },
  "belle_0.5m": {
    "hf_hub_url": "BelleGroup/train_0.5M_CN",
    "ms_hub_url": "AI-ModelScope/train_0.5M_CN"
  },
  "belle_dialog": {
    "hf_hub_url": "BelleGroup/generated_chat_0.4M",
    "ms_hub_url": "AI-ModelScope/generated_chat_0.4M"
  },
  "belle_math": {
    "hf_hub_url": "BelleGroup/school_math_0.25M",
    "ms_hub_url": "AI-ModelScope/school_math_0.25M"
  },
  "belle_multiturn": {
    "script_url": "belle_multiturn",
    "formatting": "sharegpt"
  },
  "ultra_chat": {
    "script_url": "ultra_chat",
    "formatting": "sharegpt"
  },
  "open_platypus": {
    "hf_hub_url": "garage-bAInd/Open-Platypus",
    "ms_hub_url": "AI-ModelScope/Open-Platypus"
  },
  "codealpaca": {
    "hf_hub_url": "sahil2801/CodeAlpaca-20k",
    "ms_hub_url": "AI-ModelScope/CodeAlpaca-20k"
  },
  "alpaca_cot": {
    "hf_hub_url": "QingyiSi/Alpaca-CoT",
    "ms_hub_url": "AI-ModelScope/Alpaca-CoT"
  },
  "openorca": {
    "hf_hub_url": "Open-Orca/OpenOrca",
    "ms_hub_url": "AI-ModelScope/OpenOrca",
    "columns": {
      "prompt": "question",
<<<<<<< HEAD
      "response": "Respond",
=======
      "response": "response",
>>>>>>> upstream/main
      "system": "system_prompt"
    }
  },
  "slimorca": {
    "hf_hub_url": "Open-Orca/SlimOrca",
    "formatting": "sharegpt"
  },
  "mathinstruct": {
    "hf_hub_url": "TIGER-Lab/MathInstruct",
    "ms_hub_url": "AI-ModelScope/MathInstruct",
    "columns": {
      "prompt": "instruction",
      "response": "output"
    }
  },
  "firefly": {
    "hf_hub_url": "YeungNLP/firefly-train-1.1M",
    "columns": {
      "prompt": "input",
      "response": "target"
    }
  },
  "wikiqa": {
    "hf_hub_url": "wiki_qa",
    "columns": {
      "prompt": "question",
      "response": "answer"
    }
  },
  "webqa": {
    "hf_hub_url": "suolyer/webqa",
    "ms_hub_url": "AI-ModelScope/webqa",
    "columns": {
      "prompt": "input",
      "response": "output"
    }
  },
  "webnovel": {
    "hf_hub_url": "zxbsmk/webnovel_cn",
    "ms_hub_url": "AI-ModelScope/webnovel_cn"
  },
  "nectar_sft": {
    "hf_hub_url": "AstraMindAI/SFT-Nectar",
    "ms_hub_url": "AI-ModelScope/SFT-Nectar"
  },
  "deepctrl": {
    "ms_hub_url": "deepctrl/deepctrl-sft-data"
  },
<<<<<<< HEAD
  "adgen": {
=======
  "adgen_train": {
>>>>>>> upstream/main
    "hf_hub_url": "HasturOfficial/adgen",
    "ms_hub_url": "AI-ModelScope/adgen",
    "split": "train",
    "columns": {
      "prompt": "content",
      "response": "summary"
    }
  },
  "adgen_eval": {
    "hf_hub_url": "HasturOfficial/adgen",
    "ms_hub_url": "AI-ModelScope/adgen",
    "split": "validation",
    "columns": {
      "prompt": "content",
      "response": "summary"
    }
  },
  "sharegpt_hyper": {
    "hf_hub_url": "totally-not-an-llm/sharegpt-hyperfiltered-3k",
    "formatting": "sharegpt"
  },
  "sharegpt4": {
    "hf_hub_url": "shibing624/sharegpt_gpt4",
    "ms_hub_url": "AI-ModelScope/sharegpt_gpt4",
    "formatting": "sharegpt"
  },
  "ultrachat_200k": {
    "hf_hub_url": "HuggingFaceH4/ultrachat_200k",
    "ms_hub_url": "AI-ModelScope/ultrachat_200k",
    "split": "train_sft",
    "formatting": "sharegpt",
    "columns": {
      "messages": "messages"
    },
    "tags": {
      "role_tag": "role",
      "content_tag": "content",
      "user_tag": "user",
      "assistant_tag": "assistant"
    }
  },
<<<<<<< HEAD
=======
  "infinity_instruct": {
    "hf_hub_url": "BAAI/Infinity-Instruct",
    "formatting": "sharegpt"
  },
>>>>>>> upstream/main
  "agent_instruct": {
    "hf_hub_url": "THUDM/AgentInstruct",
    "ms_hub_url": "ZhipuAI/AgentInstruct",
    "formatting": "sharegpt"
  },
  "lmsys_chat": {
    "hf_hub_url": "lmsys/lmsys-chat-1m",
    "ms_hub_url": "AI-ModelScope/lmsys-chat-1m",
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversation"
    },
    "tags": {
      "role_tag": "role",
      "content_tag": "content",
      "user_tag": "user",
      "assistant_tag": "assistant"
    }
  },
  "evol_instruct": {
    "hf_hub_url": "WizardLM/WizardLM_evol_instruct_V2_196k",
    "ms_hub_url": "AI-ModelScope/WizardLM_evol_instruct_V2_196k",
    "formatting": "sharegpt"
  },
  "glaive_toolcall_100k": {
    "hf_hub_url": "hiyouga/glaive-function-calling-v2-sharegpt",
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations",
      "tools": "tools"
    }
  },
  "cosmopedia": {
    "hf_hub_url": "HuggingFaceTB/cosmopedia",
    "columns": {
      "prompt": "prompt",
      "response": "text"
    }
  },
  "stem_zh": {
    "hf_hub_url": "hfl/stem_zh_instruction"
  },
  "ruozhiba_gpt4": {
    "hf_hub_url": "hfl/ruozhiba_gpt4_turbo"
  },
  "neo_sft": {
    "hf_hub_url": "m-a-p/neo_sft_phase2",
    "formatting": "sharegpt"
  },
  "magpie_pro_300k": {
    "hf_hub_url": "Magpie-Align/Magpie-Pro-300K-Filtered",
    "formatting": "sharegpt"
  },
  "magpie_ultra": {
    "hf_hub_url": "argilla/magpie-ultra-v0.1",
    "columns": {
      "prompt": "instruction",
<<<<<<< HEAD
      "response": "Respond"
=======
      "response": "response"
>>>>>>> upstream/main
    }
  },
  "web_instruct": {
    "hf_hub_url": "TIGER-Lab/WebInstructSub",
    "columns": {
      "prompt": "question",
      "response": "answer"
    }
  },
  "openo1_sft": {
    "hf_hub_url": "llamafactory/OpenO1-SFT",
    "ms_hub_url": "llamafactory/OpenO1-SFT",
    "columns": {
      "prompt": "prompt",
<<<<<<< HEAD
      "response": "Respond"
=======
      "response": "response"
>>>>>>> upstream/main
    }
  },
  "open_thoughts": {
    "hf_hub_url": "llamafactory/OpenThoughts-114k",
    "formatting": "sharegpt",
    "columns": {
      "messages": "messages"
    },
    "tags": {
      "role_tag": "role",
      "content_tag": "content",
      "user_tag": "user",
      "assistant_tag": "assistant",
      "system_tag": "system"
    }
  },
  "open_r1_math": {
    "hf_hub_url": "llamafactory/OpenR1-Math-94k",
    "formatting": "sharegpt",
    "columns": {
      "messages": "messages"
    },
    "tags": {
      "role_tag": "role",
      "content_tag": "content",
      "user_tag": "user",
      "assistant_tag": "assistant",
      "system_tag": "system"
    }
  },
  "chinese_r1_distill": {
    "hf_hub_url": "Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT",
    "ms_hub_url": "liucong/Chinese-DeepSeek-R1-Distill-data-110k-SFT"
  },
  "llava_1k_en": {
    "hf_hub_url": "BUAADreamer/llava-en-zh-2k",
    "subset": "en",
    "formatting": "sharegpt",
    "columns": {
      "messages": "messages",
      "images": "images"
    },
    "tags": {
      "role_tag": "role",
      "content_tag": "content",
      "user_tag": "user",
      "assistant_tag": "assistant"
    }
  },
  "llava_1k_zh": {
    "hf_hub_url": "BUAADreamer/llava-en-zh-2k",
    "subset": "zh",
    "formatting": "sharegpt",
    "columns": {
      "messages": "messages",
      "images": "images"
    },
    "tags": {
      "role_tag": "role",
      "content_tag": "content",
      "user_tag": "user",
      "assistant_tag": "assistant"
    }
  },
  "llava_150k_en": {
    "hf_hub_url": "BUAADreamer/llava-en-zh-300k",
    "subset": "en",
    "formatting": "sharegpt",
    "columns": {
      "messages": "messages",
      "images": "images"
    },
    "tags": {
      "role_tag": "role",
      "content_tag": "content",
      "user_tag": "user",
      "assistant_tag": "assistant"
    }
  },
  "llava_150k_zh": {
    "hf_hub_url": "BUAADreamer/llava-en-zh-300k",
    "subset": "zh",
    "formatting": "sharegpt",
    "columns": {
      "messages": "messages",
      "images": "images"
    },
    "tags": {
      "role_tag": "role",
      "content_tag": "content",
      "user_tag": "user",
      "assistant_tag": "assistant"
    }
  },
  "pokemon_cap": {
    "hf_hub_url": "llamafactory/pokemon-gpt4o-captions",
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations",
      "images": "images"
    }
  },
  "mllm_pt_demo": {
    "hf_hub_url": "BUAADreamer/mllm_pt_demo",
    "formatting": "sharegpt",
    "columns": {
      "messages": "messages",
      "images": "images"
    },
    "tags": {
      "role_tag": "role",
      "content_tag": "content",
      "user_tag": "user",
      "assistant_tag": "assistant"
    }
  },
  "oasst_de": {
    "hf_hub_url": "mayflowergmbh/oasst_de"
  },
  "dolly_15k_de": {
    "hf_hub_url": "mayflowergmbh/dolly-15k_de"
  },
  "alpaca-gpt4_de": {
    "hf_hub_url": "mayflowergmbh/alpaca-gpt4_de"
  },
  "openschnabeltier_de": {
    "hf_hub_url": "mayflowergmbh/openschnabeltier_de"
  },
  "evol_instruct_de": {
    "hf_hub_url": "mayflowergmbh/evol-instruct_de"
  },
  "dolphin_de": {
    "hf_hub_url": "mayflowergmbh/dolphin_de"
  },
  "booksum_de": {
    "hf_hub_url": "mayflowergmbh/booksum_de"
  },
  "airoboros_de": {
    "hf_hub_url": "mayflowergmbh/airoboros-3.0_de"
  },
  "ultrachat_de": {
    "hf_hub_url": "mayflowergmbh/ultra-chat_de"
  },
  "dpo_en_demo": {
    "file_name": "dpo_en_demo.json",
    "ranking": true,
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations",
      "chosen": "chosen",
      "rejected": "rejected"
    }
  },
  "dpo_zh_demo": {
    "file_name": "dpo_zh_demo.json",
    "ranking": true,
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations",
      "chosen": "chosen",
      "rejected": "rejected"
    }
  },
  "dpo_mix_en": {
    "hf_hub_url": "llamafactory/DPO-En-Zh-20k",
    "subset": "en",
    "ranking": true,
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations",
      "chosen": "chosen",
      "rejected": "rejected"
    }
  },
  "dpo_mix_zh": {
    "hf_hub_url": "llamafactory/DPO-En-Zh-20k",
    "subset": "zh",
    "ranking": true,
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations",
      "chosen": "chosen",
      "rejected": "rejected"
    }
  },
  "ultrafeedback": {
    "hf_hub_url": "llamafactory/ultrafeedback_binarized",
    "ms_hub_url": "llamafactory/ultrafeedback_binarized",
    "ranking": true,
    "columns": {
      "prompt": "instruction",
      "chosen": "chosen",
      "rejected": "rejected"
    }
  },
  "coig_p": {
    "hf_hub_url": "m-a-p/COIG-P",
    "ranking": true,
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations",
      "chosen": "chosen",
      "rejected": "rejected"
    }
  },
  "rlhf_v": {
    "hf_hub_url": "llamafactory/RLHF-V",
    "ranking": true,
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations",
      "chosen": "chosen",
      "rejected": "rejected",
      "images": "images"
    }
  },
  "vlfeedback": {
    "hf_hub_url": "Zhihui/VLFeedback",
    "ranking": true,
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations",
      "chosen": "chosen",
      "rejected": "rejected",
      "images": "images"
    }
  },
  "rlaif_v": {
    "hf_hub_url": "openbmb/RLAIF-V-Dataset",
    "ranking": true,
    "columns": {
      "prompt": "question",
      "chosen": "chosen",
      "rejected": "rejected",
      "images": "image"
    }
  },
  "orca_pairs": {
    "hf_hub_url": "Intel/orca_dpo_pairs",
    "ranking": true,
    "columns": {
      "prompt": "question",
      "chosen": "chosen",
      "rejected": "rejected",
      "system": "system"
    }
  },
  "hh_rlhf_en": {
    "script_url": "hh_rlhf_en",
    "ranking": true,
    "columns": {
      "prompt": "instruction",
      "chosen": "chosen",
      "rejected": "rejected",
      "history": "history"
    }
  },
  "nectar_rm": {
    "hf_hub_url": "AstraMindAI/RLAIF-Nectar",
    "ms_hub_url": "AI-ModelScope/RLAIF-Nectar",
    "ranking": true
  },
  "orca_dpo_de": {
    "hf_hub_url": "mayflowergmbh/intel_orca_dpo_pairs_de",
    "ranking": true
  },
  "kto_en_demo": {
    "file_name": "kto_en_demo.json",
    "formatting": "sharegpt",
    "columns": {
      "messages": "messages",
      "kto_tag": "label"
    },
    "tags": {
      "role_tag": "role",
      "content_tag": "content",
      "user_tag": "user",
      "assistant_tag": "assistant"
    }
  },
  "kto_mix_en": {
    "hf_hub_url": "argilla/kto-mix-15k",
    "formatting": "sharegpt",
    "columns": {
      "messages": "completion",
      "kto_tag": "label"
    },
    "tags": {
      "role_tag": "role",
      "content_tag": "content",
      "user_tag": "user",
      "assistant_tag": "assistant"
    }
  },
  "ultrafeedback_kto": {
    "hf_hub_url": "argilla/ultrafeedback-binarized-preferences-cleaned-kto",
    "ms_hub_url": "AI-ModelScope/ultrafeedback-binarized-preferences-cleaned-kto",
    "columns": {
      "prompt": "prompt",
      "response": "completion",
      "kto_tag": "label"
    }
  },
  "wiki_demo": {
    "file_name": "wiki_demo.txt",
    "columns": {
      "prompt": "text"
    }
  },
  "c4_demo": {
    "file_name": "c4_demo.jsonl",
    "columns": {
      "prompt": "text"
    }
  },
  "refinedweb": {
    "hf_hub_url": "tiiuae/falcon-refinedweb",
    "columns": {
      "prompt": "content"
    }
  },
  "redpajama_v2": {
    "hf_hub_url": "togethercomputer/RedPajama-Data-V2",
    "columns": {
      "prompt": "raw_content"
    },
    "subset": "default"
  },
  "wikipedia_en": {
    "hf_hub_url": "olm/olm-wikipedia-20221220",
    "ms_hub_url": "AI-ModelScope/olm-wikipedia-20221220",
    "columns": {
      "prompt": "text"
    }
  },
  "wikipedia_zh": {
    "hf_hub_url": "pleisto/wikipedia-cn-20230720-filtered",
    "ms_hub_url": "AI-ModelScope/wikipedia-cn-20230720-filtered",
    "columns": {
      "prompt": "completion"
    }
  },
  "pile": {
    "hf_hub_url": "monology/pile-uncopyrighted",
    "ms_hub_url": "AI-ModelScope/pile",
    "columns": {
      "prompt": "text"
    }
  },
  "skypile": {
    "hf_hub_url": "Skywork/SkyPile-150B",
    "ms_hub_url": "AI-ModelScope/SkyPile-150B",
    "columns": {
      "prompt": "text"
    }
  },
  "fineweb": {
    "hf_hub_url": "HuggingFaceFW/fineweb",
    "columns": {
      "prompt": "text"
    }
  },
  "fineweb_edu": {
    "hf_hub_url": "HuggingFaceFW/fineweb-edu",
    "columns": {
      "prompt": "text"
    }
  },
<<<<<<< HEAD
=======
  "cci3_hq": {
    "hf_hub_url": "BAAI/CCI3-HQ",
    "columns": {
      "prompt": "text"
    }
  },
  "cci3_data": {
    "hf_hub_url": "BAAI/CCI3-Data",
    "columns": {
      "prompt": "text"
    }
  },
  "cci4_base": {
    "hf_hub_url": "BAAI/CCI4.0-M2-Base-v1",
    "columns": {
      "prompt": "text"
    }
  },
  "cci4_cot": {
    "hf_hub_url": "BAAI/CCI4.0-M2-CoT-v1",
    "columns": {
      "prompt": "text"
    }
  },
  "cci4_extra": {
    "hf_hub_url": "BAAI/CCI4.0-M2-Extra-v1",
    "columns": {
      "prompt": "text"
    }
  },
>>>>>>> upstream/main
  "the_stack": {
    "hf_hub_url": "bigcode/the-stack",
    "ms_hub_url": "AI-ModelScope/the-stack",
    "columns": {
      "prompt": "content"
    }
  },
  "starcoder_python": {
    "hf_hub_url": "bigcode/starcoderdata",
    "ms_hub_url": "AI-ModelScope/starcoderdata",
    "columns": {
      "prompt": "content"
    },
    "folder": "python"
<<<<<<< HEAD
  },
  "mnli": {
    "hf_hub_url": "nyu-mll/glue",
    "subset": "mnli",
    "split": "train",
    "instruction": "Classify the following premise and hypothesis pair into entailment, neutral, and contradiction classes. Respond only with the corresponding class.",
    "columns": {
      "prompt": "premise",
      "query": "hypothesis",
      "response": "label"
    }
  },
  "mnli_eval": {
    "hf_hub_url": "nyu-mll/glue",
    "subset": "mnli",
    "split": "validation_mismatched",
    "instruction": "Classify the following premise and hypothesis pair into entailment, neutral, and contradiction classes. Respond only with the corresponding class.",
    "columns": {
      "prompt": "premise",
      "query": "hypothesis",
      "response": "label"
    }
  },
  "mnli_test": {
    "hf_hub_url": "nyu-mll/glue",
    "subset": "mnli",
    "split": "validation_matched",
    "instruction": "Classify the following premise and hypothesis pair into entailment, neutral, and contradiction classes. Respond only with the corresponding class.",
    "columns": {
      "prompt": "premise",
      "query": "hypothesis",
      "response": "label"
    }
  },
  "qqp": {
    "hf_hub_url": "nyu-mll/glue",
    "subset": "qqp",
    "split": "train",
    "instruction": "Classify the following question pair into duplicate and not_duplicate classes. Respond only with the corresponding class.",
    "columns": {
      "prompt": "question1",
      "query": "question2",
      "response": "label"
    }
  },
  "qqp_eval": {
    "hf_hub_url": "nyu-mll/glue",
    "subset": "qqp",
    "split": "validation",
    "instruction": "Classify the following question pair into duplicate and not_duplicate classes. Respond only with the corresponding class.",
    "columns": {
      "prompt": "question1",
      "query": "question2",
      "response": "label"
    }
  },
  "qnli": {
    "hf_hub_url": "nyu-mll/glue",
    "subset": "qnli",
    "split": "train",
    "instruction": "Classify the following question and sentence pair into entailment and not_entailment classes. Respond only with the corresponding class.",
    "columns": {
      "prompt": "question",
      "query": "sentence",
      "response": "label"
    }
  },
  "qnli_eval": {
    "hf_hub_url": "nyu-mll/glue",
    "subset": "qnli",
    "split": "validation",
    "instruction": "Classify the following question and sentence pair into entailment and not_entailment classes. Respond only with the corresponding class.",
    "columns": {
      "prompt": "question",
      "query": "sentence",
      "response": "label"
    }
  },
  "sst2": {
    "hf_hub_url": "nyu-mll/glue",
    "subset": "sst2",
    "split": "train",
    "instruction": "Classify the following sentence into negative and positive classes. Respond only with the corresponding class.",
    "columns": {
      "prompt": "sentence",
      "response": "label"
    }
  },
  "sst2_eval": {
    "hf_hub_url": "nyu-mll/glue",
    "subset": "sst2",
    "split": "validation",
    "instruction": "Classify the following sentence into negative and positive classes. Respond only with the corresponding class.",
    "columns": {
      "prompt": "sentence",
      "response": "label"
    }
  },
  "stsb": {
    "hf_hub_url": "nyu-mll/glue",
    "subset": "stsb",
    "split": "train",
    "instruction": "Assign a similarity score from 0 to 5 to the following sentence pair. Respond only with the corresponding score.",
    "columns": {
      "prompt": "sentence1",
      "query": "sentence2",
      "response": "label"
    }
  },
  "stsb_eval": {
    "hf_hub_url": "nyu-mll/glue",
    "subset": "stsb",
    "split": "validation",
    "instruction": "Assign a similarity score from 0 to 5 to the following sentence pair. Respond only with the corresponding score.",
    "columns": {
      "prompt": "sentence1",
      "query": "sentence2",
      "response": "label"
    }
  },
  "mrpc": {
    "hf_hub_url": "nyu-mll/glue",
    "subset": "mrpc",
    "split": "train",
    "instruction": "Classify the following sentence pair into not_equivalent and equivalent classes. Respond only with the corresponding class.",
    "columns": {
      "prompt": "sentence1",
      "query": "sentence2",
      "response": "label"
    }
  },
  "mrpc_eval": {
    "hf_hub_url": "nyu-mll/glue",
    "subset": "mrpc",
    "split": "validation",
    "instruction": "Classify the following sentence pair into not_equivalent and equivalent classes. Respond only with the corresponding class.",
    "columns": {
      "prompt": "sentence1",
      "query": "sentence2",
      "response": "label"
    }
  },
  "mrpc_test": {
    "hf_hub_url": "nyu-mll/glue",
    "subset": "mrpc",
    "split": "test",
    "instruction": "Classify the following sentence pair into not_equivalent and equivalent classes. Respond only with the corresponding class.",
    "columns": {
      "prompt": "sentence1",
      "query": "sentence2",
      "response": "label"
    }
  },
  "rte": {
    "hf_hub_url": "nyu-mll/glue",
    "subset": "rte",
    "split": "train",
    "instruction": "Classify the following sentence pair into entailment and not_entailment classes. Respond only with the corresponding class.",
    "columns": {
      "prompt": "sentence1",
      "query": "sentence2",
      "response": "label"
    }
  },
  "rte_eval": {
    "hf_hub_url": "nyu-mll/glue",
    "subset": "rte",
    "split": "validation",
    "instruction": "Classify the following sentence pair into entailment and not_entailment classes. Respond only with the corresponding class.",
    "columns": {
      "prompt": "sentence1",
      "query": "sentence2",
      "response": "label"
    }
  },
  "cola": {
    "hf_hub_url": "nyu-mll/glue",
    "subset": "cola",
    "split": "train",
    "instruction": "Classify the following sentence into unacceptable and acceptable classes. Respond only with the corresponding class.",
    "columns": {
      "prompt": "sentence",
      "response": "label"
    }
  },
  "cola_eval": {
    "hf_hub_url": "nyu-mll/glue",
    "subset": "cola",
    "split": "validation",
    "instruction": "Classify the following sentence into unacceptable and acceptable classes. Respond only with the corresponding class.",
    "columns": {
      "prompt": "sentence",
      "response": "label"
    }
  },
  "record": {
    "hf_hub_url": "rbelanec/record",
    "split": "train",
    "instruction": "Based on the following query, entities, and passage, replace the @placeholder in the query. Respond only with the correct replacement.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "record_eval": {
    "hf_hub_url": "rbelanec/record",
    "split": "validation",
    "instruction": "Based on the following query, entities, and passage, replace the @placeholder in the query. Respond only with the correct replacement.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "multirc": {
    "hf_hub_url": "rbelanec/multirc",
    "split": "train",
    "instruction": "Based on the following paragraph, question, and answer, determine whether the answer answers the question. Respond only with the corresponding true or false.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "multirc_eval": {
    "hf_hub_url": "rbelanec/multirc",
    "split": "validation",
    "instruction": "Based on the following paragraph, question, and answer, determine whether the answer answers the question. Respond only with the corresponding true or false.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "boolq": {
    "hf_hub_url": "super_glue",
    "subset": "boolq",
    "split": "train",
    "instruction": "Based on the passage, respond to the following question with true or false. Respond only with the corresponding true or false.",
    "columns": {
      "prompt": "question",
      "query": "passage",
      "response": "label"
    }
  },
  "boolq_eval": {
    "hf_hub_url": "super_glue",
    "subset": "boolq",
    "split": "validation",
    "instruction": "Based on the passage, respond to the following question with true or false. Respond only with the corresponding true or false.",
    "columns": {
      "prompt": "question",
      "query": "passage",
      "response": "label"
    }
  },
  "wic": {
    "hf_hub_url": "rbelanec/wic",
    "split": "train",
    "instruction": "Based on the context from the following sentence pair, classify the word into true and false classes, based on its meaning in both of the sentences. Respond only with the corresponding true or false.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "wic_eval": {
    "hf_hub_url": "rbelanec/wic",
    "split": "validation",
    "instruction": "Based on the context from the following sentence pair, classify the word into true and false classes, based on its meaning in both of the sentences. Respond only with the corresponding true or false.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "wsc": {
    "hf_hub_url": "rbelanec/wsc",
    "split": "train",
    "instruction": "Based on the following sentence, determine whether the pronoun marked with * * is referencing the noun marked with # #. Respond only with the corresponding true and false.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "wsc_eval": {
    "hf_hub_url": "rbelanec/wsc",
    "split": "validation",
    "instruction": "Based on the following sentence, determine whether the pronoun marked with * * is referencing the noun marked with # #. Respond only with the corresponding true and false.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "cb": {
    "hf_hub_url": "super_glue",
    "subset": "cb",
    "split": "train",
    "instruction": "Classify the following premise and hypothesis pair into entailment, contradiction, and neutral classes. Respond only with the corresponding class.",
    "columns": {
      "prompt": "premise",
      "query": "hypothesis",
      "response": "label"
    }
  },
  "cb_eval": {
    "hf_hub_url": "super_glue",
    "subset": "cb",
    "split": "validation",
    "instruction": "Classify the following premise and hypothesis pair into entailment, contradiction, and neutral classes. Respond only with the corresponding class.",
    "columns": {
      "prompt": "premise",
      "query": "hypothesis",
      "response": "label"
    }
  },
  "copa": {
    "hf_hub_url": "rbelanec/copa",
    "split": "train",
    "instruction": "Based on the following premise and choice pair, select the plausible alternative from choice pair. Respond only with the corresponding choice1 or choice2.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "copa_eval": {
    "hf_hub_url": "rbelanec/copa",
    "split": "validation",
    "instruction": "Based on the following premise and choice pair, select the plausible alternative from the choice pair. Respond only with the corresponding choice1 or choice2.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "mmlu": {
    "hf_hub_url": "rbelanec/mmlu",
    "split": "auxiliary_train",
    "instruction": "Based on the question and provided choices, select the right answer. Respond only with the corresponding choices A, B, C, and D.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "mmlu_eval": {
    "hf_hub_url": "rbelanec/mmlu",
    "split": "validation",
    "instruction": "Based on the question and provided choices, select the right answer. Respond only with the corresponding choices A, B, C, and D.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "piqa": {
    "hf_hub_url": "rbelanec/piqa",
    "split": "train",
    "instruction": "Based on the question and provided solutions, select the right solution. Respond only with the corresponding solution1 or solution2.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "piqa_eval": {
    "hf_hub_url": "rbelanec/piqa",
    "split": "validation",
    "instruction": "Based on the question and provided solutions, select the right solution. Respond only with the corresponding solution1 or solution2.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "siqa": {
    "hf_hub_url": "rbelanec/siqa",
    "split": "train",
    "instruction": "Based on the context, question and provided choices, select the right answer. Respond only with the corresponding choices A, B, and C.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "siqa_eval": {
    "hf_hub_url": "rbelanec/siqa",
    "split": "validation",
    "instruction": "Based on the context, question and provided choices, select the right answer. Respond only with the corresponding choices A, B and C.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "hellaswag": {
    "hf_hub_url": "rbelanec/hellaswag",
    "split": "train",
    "instruction": "Based on the sentence and provided endings, select the correct ending that finishes the sentence correctly. Respond only with the corresponding ending1, ending2, ending3, and ending4.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "hellaswag_eval": {
    "hf_hub_url": "rbelanec/hellaswag",
    "split": "validation",
    "instruction": "Based on the sentence and provided endings, select the correct ending that finishes the sentence correctly. Respond only with the corresponding ending1, ending2, ending3, and ending4.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "winogrande": {
    "hf_hub_url": "rbelanec/winogrande",
    "split": "train",
    "instruction": "",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "winogrande_eval": {
    "hf_hub_url": "rbelanec/winogrande",
    "split": "validation",
    "instruction": "Based on the sentence and provided options, select the best option that replaces the '_' character in the sentence. Respond only with the corresponding option1 or option2.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "openbookqa": {
    "hf_hub_url": "rbelanec/openbookqa",
    "split": "train",
    "instruction": "Based on the question and provided choices, select the right answer. Respond only with the corresponding choices A, B, C, and D.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
    "openbookqa_eval": {
    "hf_hub_url": "rbelanec/openbookqa",
    "split": "validation",
    "instruction": "Based on the question and provided choices, select the right answer. Respond only with the corresponding choices A, B, C, and D.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "math_qa": {
    "hf_hub_url": "rbelanec/math_qa",
    "split": "train",
    "instruction": "Based on the problem and provided options, select the right answer. Respond only with the corresponding choices a, b, c, d, and e.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "math_qa_eval": {
    "hf_hub_url": "rbelanec/math_qa",
    "split": "validation",
    "instruction": "Based on the problem and provided options, select the right answer. Respond only with the corresponding choices a, b, c, d, and e.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "gsm8k": {
    "hf_hub_url": "openai/gsm8k",
    "subset": "main",
    "split": "train",
    "instruction": "Answer following math question. The answer is always numerical. Mark your final answer with a newline starting with '####'.",
    "columns": {
      "prompt": "question",
      "response": "answer"
    }
  },
  "gsm8k_eval": {
    "hf_hub_url": "openai/gsm8k",
    "subset": "main",
    "split": "test",
    "instruction": "Answer following math question. The answer is always numerical. Mark your final answer with a newline starting with '####'.",
    "columns": {
      "prompt": "question",
      "response": "answer"
    }
  },
  "svamp": {
    "hf_hub_url": "rbelanec/svamp",
    "split": "train",
    "instruction": "Answer following math question. The answer is always numerical. Respond only with a single equation with parentheses (e.g., (1 + 1) = 2).",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "svamp_eval": {
    "hf_hub_url": "rbelanec/svamp",
    "split": "test",
    "instruction": "Answer following math question. The answer is always numerical. Respond only with a single equation with parentheses (e.g., (1 + 1) = 2).",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "conala": {
    "hf_hub_url": "neulab/conala",
    "subset": "curated",
    "split": "train",
    "instruction": "Based on the following instruction, generate a valid Python code that answers it.",
    "columns": {
      "prompt": "rewritten_intent",
      "response": "snippet"
    }
  },
  "conala_eval": {
    "hf_hub_url": "neulab/conala",
    "subset": "curated",
    "split": "test",
    "instruction": "Based on the following instruction, generate a valid Python code that answers it.",
    "columns": {
      "prompt": "rewritten_intent",
      "response": "snippet"
    }
  },
  "codealpacapy": {
    "hf_hub_url": "Abzu/CodeAlpacaPython",
    "split": "train",
    "instruction": "Based on the following instruction, generate a valid Python code that answers it.",
    "columns": {
      "prompt": "prompt",
      "response": "response"
    }
  },
  "codealpacapy_eval": {
    "hf_hub_url": "Abzu/CodeAlpacaPython",
    "split": "test",
    "instruction": "Based on the following instruction, generate a valid Python code that answers it.",
    "columns": {
      "prompt": "prompt",
      "response": "response"
    }
  },
  "apps": {
    "hf_hub_url": "rbelanec/apps",
    "split": "train",
    "instruction": "Based on the following instruction, generate a valid Python code that answers it.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
  },
  "apps_eval": {
    "hf_hub_url": "rbelanec/apps",
    "split": "test",
    "instruction": "Based on the following instruction, generate a valid Python code that answers it.",
    "columns": {
      "prompt": "inputs",
      "response": "targets"
    }
=======
>>>>>>> upstream/main
  }
}
